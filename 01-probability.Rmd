---
title: 'Lecture 1: Probability'
author: "Zhentao Shi"
date: "August 6, 2018"
output: pdf_document
linestretch: 1.5
fontsize: 12pt
urlcolor: blue
---

# Probability {#review-of-probability}

Human beings are awed by uncertainty in daily life. In the old days, Egyptians
consulted oracles, Hebrews inquired prophets, and Chinese counted on diviners
to interpret tortoise shell or bone cracks.
Even in today's Hong Kong fortunetellers are abundant.

Probability theory is a philosophy about uncertainty.
Over centuries, mathematicians strived to contribute to the understanding
of randomness. As measure theory matured in the early 20th century,
Russian mathematician Andrey Kolmogorov (1903-1987) laid the foundation of
modern probability theory in his book published in 1933.
The formal mathematical language is a system that allows rigorous explorations
that have made fruitful advancements, and is now widely accepted as a scientific
standard in academic and industrial research.

With the advent of big data, computer scientists have contributed a plethora of new
algorithms that are aimed at revealing patterns from seemingly random data. Machine learning
and artificial intelligence (AI) become buzz words in mass media. They
defeat best human Chess and Go players, automate
manufacturers, power self-driving vehicles, recognize human faces, and recommend
new online purchases. Behind their industrial success, statistical theory sheds
light on the behavior of these algorithms.
Modern probability theory is so far the most promising paradigm to rationalize
existing algorithms and engineer new ones.

Economics has been an empirical social science since Adam Smith (1723-1790). Many
numerical anecdotes appear in his *Wealth of Nations* published in 1776.
Ragnar Frisch
(1895-1973) and Jan Tinbergen (1903-1994), two pioneers econometricians, were awarded
 in 1969 the first Nobel Prize in economics. Econometrics provides quantitative
insights for economic data. It flourishes in management practices, from households and
firms up to governance at world level. The AI revolution is pumping fresh energy into
research and exercise of econometric methods, while its very foundation is again modern probability theory.

In this preparatory course, we will have a brief introduction of the axiomatic
probability theory along with familiar results covered in
undergraduate *probability and statistics*.



## Probability Space {#probability}


 A *sample space* $\Omega$ is a collection of all possible outcomes. It is a set
 of things.

An *event* $A$ is a subset of $\Omega$. It is something of interest on the sample space.

A $\sigma$-*field*, denoted by $\mathcal{F}$, is a collection of
  $(A_i\in \Omega)_{i \in \mathbb{N}}$  events such that

(i) $\emptyset\in\mathcal{F}$;
(ii) if an event $A\in\mathcal{F}$, then $A^{c}\in\mathcal{F}$;
(iii) if $A_{i}\in\mathcal{F}$ for $i\in\mathbb{N}$, then
    $\bigcup_{i\in\mathbb{N}}A_{i}\in\mathcal{F}$.



It is easy to show that $\Omega \in \mathcal{F}$ and $\bigcap_{i\in\mathbb{N}}A_{i}\in\mathcal{F}$.
The $\sigma$-field can be viewed as a well-organized structure built on the ground of the sample space. The pair $\left(\Omega,\mathcal{F}\right)$ is called a *measure space*.



Let $\mathcal{G} = \{B_1, B_2,\ldots\}$ be an arbitrary collection of sets, not necessarily a $\sigma$-field. We say $\mathcal{F}$ is
the smallest $\sigma$-field generated by $\mathcal{G}$ if $\mathcal{G}\subseteq \mathcal{F}$, and  $\mathcal{F}\subseteq \mathcal{\tilde{F}}$ for any  $\mathcal{\tilde{F}}$ such that $\mathcal{G}\subseteq \mathcal{\tilde{F}}$.
A *Borel $\sigma$-field* $\mathcal{R}$ is the
smallest $\sigma$-field generated by the open sets on the real line $\mathbb{R}$.




A function $\mu:(\Omega, \mathcal{F})\mapsto\left[0,\infty\right]$ is called a
    *measure* if it satisfies

(i) $\mu\left(A\right)\geq0$ for all     $A\in\mathcal{F}$;
(ii) if $A_{i}\in\mathcal{F}$, $i\in\mathbb{N}$,
are mutually disjoint, then
$$\mu\left(\bigcup_{i\in\mathbb{N}}A_{i}\right)=\sum_{i\in\mathbb{N}}\mu\left(A_{i}\right).$$

Measure can be understand as weight
    or length in our daily life experience.
    In particular, we call $\mu$ a *probability
    measure* if $\mu\left(\Omega\right)=1$. A probability measure is often denoted as $P$.
    The triple $\left(\Omega,\mathcal{F},P\right)$ is called a *probability space*.

## Random Variable

The terminology *random variable* somewhat belies its formal definition of a deterministic mapping. It is a link between
two measure spaces such that any event in the $\sigma$-field installed on the range
can be tracked back to an event in the $\sigma$-field installed
on the domain.

Formally, a function $X:\Omega\mapsto\mathbb{R}$ is
$\left(\Omega,\mathcal{F}\right)\backslash\left(\mathbb{R},\mathcal{R}\right)$
*measurable* if
$$X^{-1}\left(B\right)=\left\{ \omega\in\Omega:X\left(\omega\right)\in B\right\} \in\mathcal{F}$$
for any $B\in\mathcal{R}.$  *Random variable* is an alternative common
name for a measurable function. We say a measurable is a
*discrete random variable* if the set
$\left\{X\left(\omega\right):\omega\in\Omega\right\}$ is finite
or countable. We say it is a
*continuous random variable* if the set
$\left\{ X\left(\omega\right):\omega\in\Omega\right\}$
is uncountable.

A measurable function connects  two measurable spaces.
No probability is involved in its definition. However, if a probability measure $P$
is installed on  $(\Omega, \mathcal{F})$, the measurable function $X$
 will induce a probability measure on $(\mathbb{R},\mathcal{R})$.
It is easy to verify that $P_{X}:(\mathbb{R},\mathcal{R})\mapsto\left[0,1\right]$ is also a probability measure if defined as
    $P_{X}\left(B\right)=P\left(X^{-1}\left(B\right)\right)$
    for any $B\in\mathcal{R}$.  
(If $B_1, B_2\in \mathcal{R}$ are disjoint, then $X^{-1}(B_1), X^{-1}(B_2)\in \mathcal{F}$ are also disjoint.) This $P_{X}$ is called the probability measure
    *induced* by the measurable function $X$.
The induced probability measure $P_X$ is an offspring of the parent
probability measure $P$ though the channel of $X$.

## Distribution Function

We go back to some terms that we have learned in the undergraduate
probability course. A *(cumulative) distribution function*
$F:\mathbb{R}\mapsto [0,1]$ is defined as
$$F\left(x\right)=P\left(X\leq x\right)=
P\left(\{X\leq x\}\right)=P\left(\left\{ \omega\in\Omega:X\left(\omega\right)\leq x\right\} \right).$$
It is often abbreviated as CDF, and it has the following properties.

(i) $\lim_{x\to-\infty}F\left(x\right)=0$,
(ii) $\lim_{x\to\infty}F\left(x\right)=1$,
(iii) non-decreasing,
(iv)  right-continuous $\lim_{y\to x^{+}}F\left(y\right)=F\left(x\right).$

For continuous distribution, if there exists a function $f$ such that for all $x$,
$$F\left(x\right)=\int_{-\infty}^{x}f\left(y\right)dy,$$
 then $f$ is
    called the *probability density function* of $X$, often abbreviated as PDF.
It is easy to show that $f\left(x\right)\geq0$ and
    $\int_{a}^{b}f\left(x\right)dx=F\left(b\right)-F\left(a\right)$.

**Example** We have learned many parametric distributions like the binary distribution, the Poisson distribution,
the uniform distribution, the normal distribution, $\chi^{2}$, $t$, $F$ and so on.
They are parametric distributions, meaning that the CDF or PDF can be summarized in a few parameters.

**Example** `R` has a rich collection of distributions implemented in a unified rule:
`d` for density, `p` for probability, `q` for quantile, and `r` for random variable generation.
For instance, `dnorm`, `pnorm`, `qnorm`, and `rnorm` are the corresponding functions of the normal distribution, and the parameters $\mu$ and $\sigma$ can be specified in the arguments of the functions.

Below is a piece of `R` code for demonstration.

1. Plot the density of standard normal distribution over an equally spaced grid system `x_axis = seq(-3, 3, by = 0.01)` (black line).
2. Generate 1000 observations for $N(0,1)$. Plot the kernel density, a nonparametric estimation of the density (red line).
3. Calculate the 95-th quantile and the empirical probability of observing a value greater than the 95-th quantile.
In population, this value should be 5%. What is the number in this experiment?

```{r,cache=TRUE}
x_axis = seq(-3, 3, by = 0.01)

y = dnorm(x_axis)
plot(y = y, x=x_axis, type = "l", xlab = "value", ylab = "density")
z = rnorm(1000)
lines( density(z), col = "red")
crit = qnorm(.975)

mean( abs(z) > crit )
```
