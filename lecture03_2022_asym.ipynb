{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1635d2",
   "metadata": {},
   "source": [
    "# Basic Asymptotic Theory\n",
    "\n",
    "<!-- Qingliang covers basic asymptotic theory -->\n",
    "\n",
    "Nevertheless, we always have at hand a finite sample, and mostly it is\n",
    "difficult to increase the sample size in reality. Asymptotic theory\n",
    "rarely answers “how large is large”, and we must be cautious about the\n",
    "treacherous landscape of *asymptopia*. In the era of big data, albeit\n",
    "the sheer size of data balloons dramatically, we build more\n",
    "sophisticated models to better capture heterogeneity in the data. Large\n",
    "sample is a relative notion to the complexity of the model and\n",
    "underlying (in)dependence structure of the data.\n",
    "\n",
    "Both the classical parametric approach, which is based on hard-to-verify\n",
    "parametric assumptions, and the asymptotic approach, which is predicated\n",
    "on imaginary infinite sequences, deviate from the reality. Which\n",
    "approach is more constructive can only be judged case by case. The\n",
    "prevalence of asymptotic theory is its mathematical amenability and\n",
    "generality. The law of evolution elevates asymptotic theory to the\n",
    "throne of mathematical statistics of our time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93896fe2",
   "metadata": {},
   "source": [
    "\n",
    "## Modes of Convergence\n",
    "\n",
    "We first review what is *convergence* for a non-random sequence, which\n",
    "you learned in high school. Let $z_{1},z_{2},\\ldots$ be an infinite\n",
    "sequence of non-random variables.\n",
    "\n",
    "Convergence of this non-random sequence means that for any\n",
    "$\\varepsilon>0$, there exists an $N\\left(\\varepsilon\\right)$ such that\n",
    "for all $n>N\\left(\\varepsilon\\right)$, we have\n",
    "$\\left|z_{n}-z\\right|<\\varepsilon$. We say $z$ is the limit of $z_{n}$,\n",
    "and write $z_{n}\\to z$ or $\\lim_{n\\to\\infty}z_{n}=z$.\n",
    "\n",
    "Instead of a deterministic sequence, we are interested in the\n",
    "convergence of a sequence of random variables. Since a random variable\n",
    "is “random” thanks to the induced probability measure by the measurable\n",
    "function, we must be clear what *convergence* means. Several modes of\n",
    "convergence are widely used.\n",
    "\n",
    "We say a sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "probability to $z$, where $z$ can be either a random variable or a\n",
    "non-random constant, if for any $\\varepsilon>0$, the probability\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|<\\varepsilon\\right\\} \\to1$\n",
    "(or equivalently\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|\\geq\\varepsilon\\right\\} \\to0$)\n",
    "as $n\\to\\infty$. We can write $z_{n}\\stackrel{p}{\\to}z$ or\n",
    "$\\mathrm{plim}_{n\\to\\infty}z_{n}=z$.\n",
    "\n",
    "A sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "squared-mean to $z$, where $z$ can be either a random variable or a\n",
    "non-random constant, if $E\\left[\\left(z_{n}-z\\right)^{2}\\right]\\to0.$ It\n",
    "is denoted as $z_{n}\\stackrel{m.s.}{\\to}z$.\n",
    "\n",
    "In these definitions either\n",
    "$P\\left\\{ \\omega:\\left|z_{n}\\left(\\omega\\right)-z\\right|>\\varepsilon\\right\\}$\n",
    "or $E\\left[\\left(z_{n}-z\\right)^{2}\\right]$ is a non-random quantity,\n",
    "and it converges to 0 as a non-random sequence.\n",
    "\n",
    "Squared-mean convergence is stronger than convergence in probability.\n",
    "That is, $z_{n}\\stackrel{m.s.}{\\to}z$ implies $z_{n}\\stackrel{p}{\\to}z$\n",
    "but the converse is untrue. Here is an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8e8fd",
   "metadata": {},
   "source": [
    "\n",
    "<span id=\"eg:in_p_in_ms\"\n",
    "label=\"eg:in_p_in_ms\">\\[eg:in_p\\_in_ms\\]</span>$(z_{n})$ is a sequence\n",
    "of binary random variables: $z_{n}=\\sqrt{n}$ with probability $1/n$, and\n",
    "$z_{n}=0$ with probability $1-1/n$. Then $z_{n}\\stackrel{p}{\\to}0$ but\n",
    "$z_{n}\\stackrel{m.s.}{\\nrightarrow}0$. To verify these claims, notice\n",
    "that for any $\\varepsilon>0$, we have\n",
    "$P\\left(\\omega:\\left|z_{n}\\left(\\omega\\right)-0\\right|<\\varepsilon\\right)=P\\left(\\omega:z_{n}\\left(\\omega\\right)=0\\right)=1-1/n\\rightarrow1$\n",
    "and thereby $z_{n}\\stackrel{p}{\\to}0$. On the other hand,\n",
    "$E\\left[\\left(z_{n}-0\\right)^{2}\\right]=n\\cdot1/n+0\\cdot(1-1/n)=1\\nrightarrow0,$\n",
    "so $z_{n}\\stackrel{m.s.}{\\nrightarrow}0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d9d30",
   "metadata": {},
   "source": [
    "\n",
    "Example\n",
    "<a href=\"#eg:in_p_in_ms\" data-reference-type=\"ref\" data-reference=\"eg:in_p_in_ms\">[eg:in_p_in_ms]</a>\n",
    "highlights the difference between the two modes of convergence.\n",
    "Convergence in probability does not count what happens on a subset in\n",
    "the sample space of small probability. Squared-mean convergence deals\n",
    "with the average over the entire probability space. If a random variable\n",
    "can take a wild value, with small probability though, it may blow away\n",
    "the squared-mean convergence. On the contrary, such irregularity does\n",
    "not undermine convergence in probability.\n",
    "\n",
    "Both convergence in probability and squared-mean convergence are about\n",
    "convergence of random variables to a target random variable or constant.\n",
    "That is, the distribution of $z_{n}-z$ is concentrated around 0 as\n",
    "$n\\to\\infty$. Instead, *convergence in distribution* is about the\n",
    "convergence of CDF, but not the random variable. Let\n",
    "$F_{z_{n}}\\left(\\cdot\\right)$ be the CDF of $z_{n}$ and\n",
    "$F_{z}\\left(\\cdot\\right)$ be the CDF of $z$.\n",
    "\n",
    "We say a sequence of random variables $\\left(z_{n}\\right)$ converges in\n",
    "distribution to a random variable $z$ if\n",
    "$F_{z_{n}}\\left(a\\right)\\to F_{z}\\left(a\\right)$ as $n\\to\\infty$ at each\n",
    "point $a\\in\\mathbb{R}$ such that where $F_{z}\\left(\\cdot\\right)$ is\n",
    "continuous. We write $z_{n}\\stackrel{d}{\\to}z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267daca",
   "metadata": {},
   "source": [
    "\n",
    "Convergence in distribution is the weakest mode. If\n",
    "$z_{n}\\stackrel{p}{\\to}z$, then $z_{n}\\stackrel{d}{\\to}z$. The converse\n",
    "is not true in general, unless $z$ is a non-random constant (A constant\n",
    "$z$ can be viewed as a degenerate random variables, with a corresponding\n",
    "“CDF” $F_{z}\\left(\\cdot\\right)=1\\left\\{ \\cdot\\geq z\\right\\}$.\n",
    "\n",
    "Let $x\\sim N\\left(0,1\\right)$. If $z_{n}=x+1/n$, then\n",
    "$z_{n}\\stackrel{p}{\\to}x$ and of course $z_{n}\\stackrel{d}{\\to}x$.\n",
    "However, if $z_{n}=-x+1/n$, or $z_{n}=y+1/n$ where\n",
    "$y\\sim N\\left(0,1\\right)$ is independent of $x$, then\n",
    "$z_{n}\\stackrel{d}{\\to}x$ but $z_{n}\\stackrel{p}{\\nrightarrow}x$.\n",
    "\n",
    "$(z_{n})$ is a sequence of binary random variables: $z_{n}=n$ with\n",
    "probability $1/\\sqrt{n}$, and $z_{n}=0$ with probability $1-1/\\sqrt{n}$.\n",
    "Then $z_{n}\\stackrel{d}{\\to}z=0.$ Because\n",
    "$$F_{z_{n}}\\left(a\\right)=\\begin{cases}\n",
    "0 & a<0\\\\\n",
    "1-1/\\sqrt{n} & 0\\leq a\\leq n\\\\\n",
    "1 & a\\geq n\n",
    "\\end{cases}.$$\n",
    "$F_{z}\\left(a\\right)=\\begin{cases} 0, & a<0\\\\ 1 & a\\geq0 \\end{cases}$.\n",
    "It is easy to verify that $F_{z_{n}}\\left(a\\right)$ converges to\n",
    "$F_{z}\\left(a\\right)$ *pointwisely* on each point in\n",
    "$\\left(-\\infty,0\\right)\\cup\\left(0,+\\infty\\right)$, where\n",
    "$F_{z}\\left(a\\right)$ is continuous.\n",
    "\n",
    "So far we have talked about convergence of scalar variables. These three\n",
    "modes of converges can be easily generalized to random vectors. In\n",
    "particular, the *Cramer-Wold device* collapses a random vector into a\n",
    "random vector via arbitrary linear combination. We say a sequence of\n",
    "$K$-dimensional random vectors $\\left(z_{n}\\right)$ converge in\n",
    "distribution to $z$ if $\\lambda'z_{n}\\stackrel{d}{\\to}\\lambda'z$ for any\n",
    "$\\lambda\\in\\mathbb{R}^{K}$ and $\\left\\Vert \\lambda\\right\\Vert _{2}=1.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1a99f",
   "metadata": {},
   "source": [
    "\n",
    "## Law of Large Numbers\n",
    "\n",
    "(Weak) law of large numbers (LLN) is a collection of statements about\n",
    "convergence in probability of the sample average to its population\n",
    "counterpart. The basic form of LLN is:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}(z_{i}-E[z_{i}])\\stackrel{p}{\\to}0\n",
    "$$ \n",
    "as $n\\to\\infty$. Various versions of LLN work under different assumptions\n",
    "about some features and/or dependence of the underlying random\n",
    "variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a10c7",
   "metadata": {},
   "source": [
    "\n",
    "### Cherbyshev LLN\n",
    "\n",
    "We illustrate LLN by the simple example of Chebyshev LLN, which can be\n",
    "proved by elementary calculation. It utilizes the *Chebyshev\n",
    "inequality*.\n",
    "\n",
    "-   *Chebyshev inequality*: If a random variable $x$ has a finite second\n",
    "    moment $E\\left[x^{2}\\right]<\\infty$, then we have\n",
    "    $P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[x^{2}\\right]/\\varepsilon^{2}$\n",
    "    for any constant $\\varepsilon>0$.\n",
    "\n",
    "Show that if $r_{2}\\geq r_{1}\\geq1$, then\n",
    "$E\\left[\\left|x\\right|^{r_{2}}\\right]<\\infty$ implies\n",
    "$E\\left[\\left|x\\right|^{r_{1}}\\right]<\\infty.$ (Hint: use Holder’s\n",
    "inequality.)\n",
    "\n",
    "The Chebyshev inequality is a special case of the *Markov inequality*.\n",
    "\n",
    "-   *Markov inequality*: If a random variable $x$ has a finite $r$-th\n",
    "    absolute moment $E\\left[\\left|x\\right|^{r}\\right]<\\infty$ for some\n",
    "    $r\\ge1$, then we have\n",
    "    $P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} \\leq E\\left[\\left|x\\right|^{r}\\right]/\\varepsilon^{r}$\n",
    "    any constant $\\varepsilon>0$.\n",
    "\n",
    "It is easy to verify the Markov inequality.\n",
    "$$\n",
    "\\begin{aligned}E\\left[\\left|x\\right|^{r}\\right] & =\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}+\\int_{\\left|x\\right|\\leq\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n",
    " & \\geq\\int_{\\left|x\\right|>\\varepsilon}\\left|x\\right|^{r}dF_{X}\\\\\n",
    " & \\geq\\varepsilon^{r}\\int_{\\left|x\\right|>\\varepsilon}dF_{X}=\\varepsilon^{r}P\\left\\{ \\left|x\\right|>\\varepsilon\\right\\} .\n",
    "\\end{aligned}\n",
    "$$ \n",
    "Rearrange the above inequality and we obtain the Markov\n",
    "inequality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3d639",
   "metadata": {},
   "source": [
    "\n",
    "Let the *partial sum* $S_{n}=\\sum_{i=1}^{n}x_{i}$, where\n",
    "$\\mu_{i}=E\\left[x_{i}\\right]$ and\n",
    "$\\sigma_{i}^{2}=\\mathrm{var}\\left[x_{i}\\right]$. We apply the Chebyshev\n",
    "inequality to the sample mean\n",
    "$z_{n}=\\overline{x}-\\bar{\\mu}=n^{-1}\\left(S_{n}-E\\left[S_{n}\\right]\\right)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P\\left\\{ \\left|z_{n}\\right|\\geq\\varepsilon\\right\\}  & =P\\left\\{ n^{-1}\\left|S_{n}-E\\left[S_{n}\\right]\\right|\\geq\\varepsilon\\right\\} \\\\\n",
    " & \\leq E\\left[\\left(n^{-1}\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)\\right)^{2}\\right]/\\varepsilon^{2} \\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\left\\{ E\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{i}\\right)^{2}\\right]+\\sum_{i=1}^{n}\\sum_{j\\neq i}E\\left[\\left(x_{i}-\\mu_{i}\\right)\\left(x_{j}-\\mu_{j}\\right)\\right]\\right\\} \\\\\n",
    " & =\\left(n\\varepsilon\\right)^{-2}\\left\\{ \\sum_{i=1}^{n}\\mathrm{var}\\left(x_{i}\\right)+\\sum_{i=1}^{n}\\sum_{j\\neq i}\\mathrm{cov}\\left(x_{i},x_{j}\\right)\\right\\}.\n",
    " \\end{aligned}\n",
    " $$\n",
    " \n",
    "Convergence in probability holds if the right-hand side shrinks to 0 as\n",
    "$n\\to\\infty$. For example, If $x_{1},\\ldots,x_{n}$ are iid with\n",
    "$\\mathrm{var}\\left(x_{1}\\right)=\\sigma^{2}$, then the RHS of\n",
    "(<a href=\"#eq:cheby_mean\" data-reference-type=\"ref\" data-reference=\"eq:cheby_mean\">[eq:cheby_mean]</a>)\n",
    "is\n",
    "$\\left(n\\varepsilon\\right)^{-2}\\left(n\\sigma^{2}\\right)=o\\left(n^{-1}\\right)\\to0$.\n",
    "This result gives the Chebyshev LLN:\n",
    "\n",
    "-   Chebyshev LLN: If $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    iid observations, $E\\left[z_{1}\\right]=\\mu$ , and\n",
    "    $\\sigma^{2}=\\mathrm{var}\\left[z_{1}\\right]<\\infty$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu.$\n",
    "\n",
    "The convergence in probability can be indeed maintained under much more\n",
    "general conditions than under iid case. The random variables in the\n",
    "sample do not have to be identically distributed, and they do not have\n",
    "to be independent either.\n",
    "\n",
    "Consider an inid (independent but non-identically distributed) sample\n",
    "$\\left(x_{1},\\ldots,x_{n}\\right)$ with $E\\left[x_{i}\\right]=0$ and\n",
    "$\\mathrm{var}\\left[x_{i}\\right]=\\sqrt{n}c$ for some constant $c>0$. Use\n",
    "the Chebyshev inequality to show that\n",
    "$n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0$.\n",
    "\n",
    "Consider the time series moving average model\n",
    "$x_{i}=\\varepsilon_{i}+\\theta\\varepsilon_{i-1}$ for $i=1,\\ldots,n$,\n",
    "where $\\left|\\theta\\right|<1$, $E\\left[\\varepsilon_{i}\\right]=0$,\n",
    "$\\mathrm{var}\\left[\\varepsilon_{i}\\right]=\\sigma^{2}$, and\n",
    "$\\left(\\varepsilon_{i}\\right)_{i=0}^{n}$ iid. Use the Chebyshev\n",
    "inequality to show that $n^{-1}\\sum_{i=1}^{n}x_{i}\\stackrel{p}{\\to}0$.\n",
    "\n",
    "Another useful LLN is the *Kolmogorov LLN*. Since its derivation\n",
    "requires more advanced knowledge of probability theory, we state the\n",
    "result without proof.\n",
    "\n",
    "-   Kolmogorov LLN: If $\\left(z_{1},\\ldots,z_{n}\\right)$ is a sample of\n",
    "    iid observations and $E\\left[z_{1}\\right]=\\mu$ exists, then\n",
    "    $\\frac{1}{n}\\sum_{i=1}^{n}z_{i}\\stackrel{p}{\\to}\\mu$.\n",
    "\n",
    "Compared with the Chebyshev LLN, the Kolmogorov LLN only requires the\n",
    "existence of the population mean, but not any higher moments. On the\n",
    "other hand, iid is essential for the Kolmogorov LLN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a8d47",
   "metadata": {},
   "source": [
    "\n",
    "Consider three distributions: standard normal $N\\left(0,1\\right)$,\n",
    "$t\\left(2\\right)$ (zero mean, infinite variance), and the Cauchy\n",
    "distribution (no moments exist). We plot paths of the sample average\n",
    "with $n=2^{1},2^{2},\\ldots,2^{20}$. We will see that the sample averages\n",
    "of $N\\left(0,1\\right)$ and $t\\left(2\\right)$ converge, but that of the\n",
    "Cauchy distribution does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b85aa",
   "metadata": {},
   "source": [
    "### Large of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a67e98",
   "metadata": {},
   "source": [
    "This script demonstrates the law of large numbers (LLN) along with the underlying assumptions.\n",
    "\n",
    "Write a function to generate the sample mean given the sample size $n$ and the distribution.\n",
    "We allow three distributions, namely, $N(0,1)$, $t(2)$ and Cauchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059833a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sample.mean = function( n, distribution ){\n",
    "  # get sample mean for a given distribution\n",
    "  if (distribution == \"normal\"){ y = rnorm( n ) } \n",
    "  else if (distribution == \"t2\") {y = rt(n, 2) }\n",
    "  else if (distribution == \"cauchy\") {y = rcauchy(n) }\n",
    "  return( mean(y) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ba664",
   "metadata": {},
   "source": [
    "This function plots the sample mean over the path of geometrically increasing sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf64fb6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "LLN.plot = function(distribution){\n",
    "  # draw the sample mean graph\n",
    "  ybar = matrix(0, length(NN), 3 )\n",
    "  for (rr in 1:3){\n",
    "    for ( ii in 1:length(NN)){\n",
    "      n = NN[ii]; ybar[ii, rr] = sample.mean(n, distribution)\n",
    "    }  \n",
    "  }\n",
    "  matplot(ybar, type = \"l\", ylab = \"mean\", xlab = \"\", \n",
    "       lwd = 1, lty = 1, main = distribution)\n",
    "  abline(h = 0, lty = 2)\n",
    "  return(ybar)\n",
    "}\n",
    "# calculation\n",
    "NN = 2^(1:20); set.seed(2020-10-7); par(mfrow = c(3,1))\n",
    "l1 = LLN.plot(\"normal\"); l2 = LLN.plot(\"t2\"); l3 = LLN.plot(\"cauchy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1d65f",
   "metadata": {},
   "source": [
    "\n",
    "## Central Limit Theorem\n",
    "\n",
    "The central limit theorem (CLT) is a collection of probability results\n",
    "about the convergence in distribution to a stable distribution. The\n",
    "limiting distribution is usually the Gaussian distribution. The basic\n",
    "form of the CLT is:\n",
    "\n",
    "-   *Under some conditions to be spelled out*, the sample average of\n",
    "    *zero-mean* random variables $\\left(z_{1},\\ldots,z_{n}\\right)$\n",
    "    multiplied by $\\sqrt{n}$ satisfies\n",
    "    $$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}z_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)$$\n",
    "    as $n\\to\\infty$.\n",
    "\n",
    "Various versions of CLT work under different assumptions about the\n",
    "random variables. *Lindeberg-Levy CLT* is the simplest CLT.\n",
    "\n",
    "-   If the sample $\\left(x_{1},\\ldots,x_{n}\\right)$ is iid,\n",
    "    $E\\left[x_{1}\\right]=0$ and\n",
    "    $\\mathrm{var}\\left[x_{1}\\right]=\\sigma^{2}<\\infty$, then\n",
    "    $\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}x_{i}\\stackrel{d}{\\to}N\\left(0,\\sigma^{2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012222b1",
   "metadata": {},
   "source": [
    "\n",
    "This is a simulated example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5cc8d",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "The sample size is chosen as $2^x$, where $x=1:20$. We have the following observations.\n",
    "* When the distribution is $N(0,1)$, the Chebyshev LLN works. The sample mean converges fast.\n",
    "* When the distribution is $t(2)$, which has zero mean but infinite variance, the Kolmogorov LLN works. The sample mean still converges, though more slowly than the $N(0,1)$ case.\n",
    "* The Cauchy distribution has no moment at any order. The sample mean does not converge no matter how large is the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33978c0b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Z_fun = function(n, distribution){\n",
    "  if (distribution == \"normal\"){\n",
    "      z = sqrt(n) * mean(rnorm(n))\n",
    "\t} else if (distribution == \"chisq2\") {\n",
    "      df = 2; \n",
    "      x = rchisq(n,2)\n",
    "      z = sqrt(n) * ( mean(x) - df ) / sqrt(2*df)\n",
    "      }\n",
    "  return (z)\n",
    "}\n",
    "CLT_plot = function(n, distribution){\n",
    "  Rep = 10000\n",
    "  ZZ = rep(0, Rep)\n",
    "  for (i in 1:Rep) {ZZ[i] = Z_fun(n, distribution)}\n",
    "\n",
    "  xbase = seq(-4.0, 4.0, length.out = 100)\n",
    "  hist( ZZ, breaks = 100, freq = FALSE, \n",
    "    xlim = c( min(xbase), max(xbase) ),\n",
    "    main = paste0(\"hist with sample size \", n) )\n",
    "  lines(x = xbase, y = dnorm(xbase), col = \"red\")\n",
    "  return (ZZ)\n",
    "}\n",
    "\n",
    "par(mfrow = c(3,1))\n",
    "phist = CLT_plot(2, \"chisq2\")\n",
    "phist = CLT_plot(10, \"chisq2\")\n",
    "phist = CLT_plot(100, \"chisq2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a39eb0",
   "metadata": {},
   "source": [
    "\n",
    "## Tools for Transformations\n",
    "\n",
    "-   Continuous mapping theorem 1: If $x_{n}\\stackrel{p}{\\to}a$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous at $a$, then\n",
    "    $f\\left(x_{n}\\right)\\stackrel{p}{\\to}f\\left(a\\right)$.\n",
    "\n",
    "-   Continuous mapping theorem 2: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $f\\left(\\cdot\\right)$ is continuous almost surely on the support of\n",
    "    $x$, then $f\\left(x_{n}\\right)\\stackrel{d}{\\to}f\\left(x\\right)$.\n",
    "\n",
    "-   Slutsky’s theorem: If $x_{n}\\stackrel{d}{\\to}x$ and\n",
    "    $y_{n}\\stackrel{p}{\\to}a$, then\n",
    "\n",
    "    -   $x_{n}+y_{n}\\stackrel{d}{\\to}x+a$\n",
    "\n",
    "    -   $x_{n}y_{n}\\stackrel{d}{\\to}ax$\n",
    "\n",
    "    -   $x_{n}/y_{n}\\stackrel{d}{\\to}x/a$ if $a\\neq0$.\n",
    "\n",
    "Slutsky’s theorem consists of special cases of the continuous mapping\n",
    "theorem 2. Only because the addition, multiplication and division are\n",
    "encountered so frequently in practice, we list it as a separate theorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953d3b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:r]",
   "language": "R",
   "name": "conda-env-r-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
