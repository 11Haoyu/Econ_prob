{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Notation: $\\mathbf{X}$ denotes a random variable or random vector.\n",
    "$\\mathbf{x}$ is its realization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc42d5",
   "metadata": {},
   "source": [
    "\n",
    "A *hypothesis* is a statement about the parameter space $\\Theta$.\n",
    "Hypothesis testing checks whether the data support a *null hypothesis*\n",
    "$\\Theta_{0}$, which is a subset of $\\Theta$ of interest. Ideally the\n",
    "null hypothesis should be suggested by scientific theory. The\n",
    "*alternative hypothesis* $\\Theta_{1}=\\Theta\\backslash\\Theta_{0}$ is the\n",
    "complement of $\\Theta_{0}$. Based on the observed evidence, hypothesis\n",
    "testing decides to accept or reject the null hypothesis. If the null\n",
    "hypothesis is rejected by the data, it implies that from the statistical\n",
    "perspective the data is incompatible with the proposed scientific\n",
    "theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790ca",
   "metadata": {},
   "source": [
    "\n",
    "In this chapter, we will first introduce the idea and practice of\n",
    "hypothesis testing and the related confidence interval. While we mainly\n",
    "focus on the frequentist interpretation of hypothesis, we briefly\n",
    "discuss the Bayesian approach to statistical decision. As an application\n",
    "of the testing procedures to the linear regression model, we elaborate\n",
    "how to test a linear or nonlinear hypothesis of the slope coefficients\n",
    "based on the unrestricted or restricted OLS estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82794aef",
   "metadata": {},
   "source": [
    "\n",
    "## Testing\n",
    "\n",
    "### Decision Rule and Errors\n",
    "\n",
    "If $\\Theta_{0}$ is a singleton, we call it a *simple hypothesis*;\n",
    "otherwise we call it a *composite hypothesis*. For example, if the\n",
    "parameter space $\\Theta=\\mathbb{R}$, then $\\Theta_{0}=\\left\\{ 0\\right\\}$\n",
    "(or equivalently $\\theta_{0}=0$) is a simple hypothesis, whereas\n",
    "$\\Theta_{0}=(-\\infty,0]$ (or equivalently $\\theta_{0}\\leq0$) is a\n",
    "composite hypothesis.\n",
    "\n",
    "A *test function* is a mapping\n",
    "$$\\phi:\\mathcal{X}^{n}\\mapsto\\left\\{ 0,1\\right\\} ,$$ where $\\mathcal{X}$\n",
    "is the sample space. The null hypothesis is accepted if\n",
    "$\\phi\\left(\\mathbf{x}\\right)=0$, or rejected if\n",
    "$\\phi\\left(\\mathbf{x}\\right)=1$. We call the set\n",
    "$A_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi_{\\theta}\\left(\\mathbf{x}\\right)=0\\right\\}$\n",
    "the *acceptance region*, and its complement\n",
    "$R_{\\phi}=\\left\\{ \\mathbf{x}\\in\\mathcal{X}^{n}:\\phi\\left(\\mathbf{x}\\right)=1\\right\\}$\n",
    "the *rejection region.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d13a15",
   "metadata": {},
   "source": [
    "\n",
    "The *power function* of a test $\\phi$ is\n",
    "$$\\beta\\left(\\theta\\right)=P_{\\theta}\\left\\{ \\phi\\left(\\mathbf{X}\\right)=1\\right\\} =E_{\\theta}\\left[\\phi\\left(\\mathbf{X}\\right)\\right].$$\n",
    "The power function measures the probability that the test function\n",
    "rejects the null when the data is generated under the true parameter\n",
    "$\\theta$, reflected in $P_{\\theta}$ and $E_{\\theta}$.\n",
    "\n",
    "The *power* of a test for some $\\theta\\in\\Theta_{1}$ is the value of\n",
    "$\\beta\\left(\\theta\\right)$. The *size* of the test is\n",
    "$\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right).$ Notice that the\n",
    "definition of power depends on a $\\theta$ in the alternative hypothesis\n",
    "$\\Theta_{1}$, whereas that of size is independent of $\\theta$ due to the\n",
    "supremum over the set of null $\\Theta_{0}$. The *level* of a test is any\n",
    "value $\\alpha\\in\\left(0,1\\right)$ such that\n",
    "$\\alpha\\geq\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)$, which is\n",
    "often used when it is difficult to attain the exact supremum. A test of\n",
    "size $\\alpha$ is also of level $\\alpha$ or bigger; while a test of level\n",
    "$\\alpha$ must have size smaller or equal to $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ba15b",
   "metadata": {},
   "source": [
    "\n",
    "The concept of *level* is useful if we do not have sufficient\n",
    "information to derive the exact size of a test. If\n",
    "$\\left(X_{1i},X_{2i}\\right)_{i=1}^{n}$ are randomly drawn from some\n",
    "unknown joint distribution, but we know the marginal distribution is\n",
    "$X_{ji}\\sim N\\left(\\theta_{j},1\\right)$, for $j=1,2$. In order to test\n",
    "the joint hypothesis $\\theta_{1}=\\theta_{2}=0$, we can construct a test\n",
    "function\n",
    "$$\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)=1\\left\\{ \\left\\{ \\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\cup\\left\\{ \\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right\\} \\right\\} ,$$\n",
    "where $z_{1-\\alpha/4}$ is the $\\left(1-\\alpha/4\\right)$-th quantile of\n",
    "the standard normal distribution. The level of this test is\n",
    "$$\\begin{aligned}P\\left(\\phi_{\\theta_{1}=\\theta_{2}=0}\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)\\right) & \\leq P\\left(\\sqrt{n}\\left|\\overline{X}_{1}\\right|\\geq z_{1-\\alpha/4}\\right)+P\\left(\\sqrt{n}\\left|\\overline{X}_{2}\\right|\\geq z_{1-\\alpha/4}\\right)\\\\\n",
    " & =\\alpha/2+\\alpha/2=\\alpha.\n",
    "\\end{aligned}$$ where the inequality follows by the *Bonferroni\n",
    "inequality*\n",
    "$$P\\left(A\\cup B\\right)\\leq P\\left(A\\right)+P\\left(B\\right).$$ (The\n",
    "seemingly trivial Bonferroni inequality is useful in many proofs of\n",
    "probability results.) Therefore, the level of\n",
    "$\\phi\\left(\\mathbf{X}_{1},\\mathbf{X}_{2}\\right)$ is $\\alpha$, but the\n",
    "exact size is unknown without the knowledge of the joint distribution.\n",
    "(Even if we know the correlation of $X_{1i}$ and $X_{2i}$, putting two\n",
    "marginally normal distributions together does not make a jointly normal\n",
    "vector in general.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaa581",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "                       accept $H_{0}$     reject $H_{0}$\n",
    "      $H_{0}$ true    correct decision     Type I error\n",
    "      $H_{0}$ false    Type II error     correct decision\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Actions, States and Consequences\n",
    "\n",
    "-   The *probability of committing Type I error* is\n",
    "    $\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{0}$.\n",
    "\n",
    "-   The *probability of committing Type II error* is\n",
    "    $1-\\beta\\left(\\theta\\right)$ for some $\\theta\\in\\Theta_{1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c7d4",
   "metadata": {},
   "source": [
    "\n",
    "The philosophy on hypothesis testing has been debated for centuries. At\n",
    "present the prevailing framework in statistics textbooks is the\n",
    "*frequentist perspective*. A frequentist views the parameter as a fixed\n",
    "constant. They keep a conservative attitude about the Type I error: Only\n",
    "if overwhelming evidence is demonstrated shall a researcher reject the\n",
    "null. Under the principle of protecting the null hypothesis, a desirable\n",
    "test should have a small level. Conventionally we take $\\alpha=0.01,$\n",
    "0.05 or 0.1. We say a test is *unbiased* if\n",
    "$\\beta\\left(\\theta\\right)>\\sup_{\\theta\\in\\Theta_{0}}\\beta\\left(\\theta\\right)$\n",
    "for all $\\theta\\in\\Theta_{1}$. There can be many tests of correct size.\n",
    "\n",
    "A trivial test function\n",
    "$\\phi(\\mathbf{x})=1\\left\\{ 0\\leq U\\leq\\alpha\\right\\}$ for all\n",
    "$\\theta\\in\\Theta$, where $U$ is a random variable from a uniform\n",
    "distribution on $\\left[0,1\\right]$, has correct size $\\alpha$ but no\n",
    "non-trivial power at the alternative. On the other extreme, the trivial\n",
    "test function $\\phi\\left(\\mathbf{x}\\right)=1$ for all $\\mathbf{x}$\n",
    "enjoys the biggest power but suffers incorrect size.\n",
    "\n",
    "Usually, we design a test by proposing a test statistic\n",
    "$T_{n}:\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{+}$ and a critical value\n",
    "$c_{1-\\alpha}$. Given $T_{n}$ and $c_{1-\\alpha}$, we write the test\n",
    "function as\n",
    "$$\\phi\\left(\\mathbf{X}\\right)=1\\left\\{ T_{n}\\left(\\mathbf{X}\\right)>c_{1-\\alpha}\\right\\} .$$\n",
    "To ensure such a $\\phi\\left(\\mathbf{x}\\right)$ has correct size, we need\n",
    "to figure out the distribution of $T_{n}$ under the null hypothesis\n",
    "(called the *null distribution*), and choose a critical value\n",
    "$c_{1-\\alpha}$ according to the null distribution and the desirable size\n",
    "or level $\\alpha$.\n",
    "\n",
    "Another commonly used indicator in hypothesis testing is $p$-value:\n",
    "$$\\sup_{\\theta\\in\\Theta_{0}}P_{\\theta}\\left\\{ T_{n}\\left(\\mathbf{x}\\right)\\leq T_{n}\\left(\\mathbf{X}\\right)\\right\\} .$$\n",
    "In the above expression, $T_{n}\\left(\\mathbf{x}\\right)$ is the realized\n",
    "value of the test statistic $T_{n}$, while\n",
    "$T_{n}\\left(\\mathbf{X}\\right)$ is the random variable generated by\n",
    "$\\mathbf{X}$ under the null $\\theta\\in\\Theta_{0}$. The interpretation of\n",
    "the $p$-value is tricky. $p$-value is the probability that we observe\n",
    "$T_{n}(\\mathbf{X})$ being greater than the realized $T_{n}(\\mathbf{x})$\n",
    "if the null hypothesis is true.\n",
    "\n",
    "$p$-value is *not* the probability that the null hypothesis is true.\n",
    "Under the frequentist perspective, the null hypothesis is either true or\n",
    "false, with certainty. The randomness of a test comes only from\n",
    "sampling, not from the hypothesis. $p$-value measures whether the\n",
    "dataset is compatible with the null hypothesis. $p$-value is closely\n",
    "related to the corresponding test. When $p$-value is smaller than the\n",
    "specified test size $\\alpha$, the test rejects the null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21814f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "Applied econometrics is a field obsessed of hypothesis testing, in the\n",
    "hope to establish at least statistical association and ideally\n",
    "causality. Hypothesis testing is a fundamentally important topic in\n",
    "statistics. The states and the decisions in Table\n",
    "<a href=\"#tab:Decisions-and-States\" data-reference-type=\"ref\" data-reference=\"tab:Decisions-and-States\">[tab:Decisions-and-States]</a>\n",
    "remind us the intrinsic connections with game theory in economics. I, a\n",
    "game player, plays a sequential game against the “nature”.\n",
    "\n",
    "Step0:  \n",
    "The parameter space $\\Theta$ is partitioned into the null hypothesis\n",
    "$\\Theta_{0}$ and the alternative hypothesis $\\Theta_{1}$ according to a\n",
    "scientific theory.\n",
    "\n",
    "Step1:  \n",
    "Before I observe the data, I design a test function $\\phi$ according to\n",
    "$\\Theta_{0}$ and $\\Theta_{1}$. In game theory terminology, the\n",
    "contingency plan $\\phi$ is my *strategy*.\n",
    "\n",
    "Step2:  \n",
    "Once I observe the fixed data $\\mathbf{x}$, I act according to the\n",
    "instruction of $\\phi\\left(\\mathbf{x}\\right)$ — either accept\n",
    "$\\Theta_{0}$ or reject $\\Theta_{0}$.\n",
    "\n",
    "Step3:  \n",
    "Nature reveals the true parameter $\\theta^{*}$ behind $\\mathbf{x}$. Then\n",
    "I can evaluate the gain/loss of my decision\n",
    "$\\phi\\left(\\mathbf{x}\\right)$.\n",
    "\n",
    "When the loss function (negative payoff) is specified as\n",
    "$$\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)=\\phi\\left(\\mathbf{x}\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\phi\\left(\\mathbf{x}\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} ,$$\n",
    "the randomness of the data will incur the risk (expected loss)\n",
    "$$\\mathscr{R}\\left(\\theta,\\phi\\right)=E\\left[\\mathscr{L}\\left(\\theta,\\phi\\left(\\mathbf{x}\\right)\\right)\\right]=\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} .$$\n",
    "I am a rational person. I understand the structure of the game and I\n",
    "want to do a good job in Step 1 in designing my strategy. I want to\n",
    "minimize my risk.\n",
    "\n",
    "If I am a frequentist, one and only one of\n",
    "$1\\left\\{ \\theta\\in\\Theta_{0}\\right\\}$ and\n",
    "$1\\left\\{ \\theta\\in\\Theta_{1}\\right\\}$ can happen. An unbiased test\n",
    "makes sure\n",
    "$\\sup_{\\theta\\in\\Theta_{0}}\\beta_{\\phi}\\left(\\theta\\right)\\leq\\alpha$.\n",
    "When many tests are unbiased, ideally I would like to pick the best one.\n",
    "If it exists, in a class $\\Psi_{\\alpha}$ of unbiased tests of size\n",
    "$\\alpha$ the uniformly most power test $\\phi^{*}$ satisfies\n",
    "$\\mathscr{R}\\left(\\theta,\\phi^{*}\\right)\\geq\\sup_{\\phi\\in\\Psi_{\\alpha}}\\mathscr{R}\\left(\\theta,\\phi\\right)$\n",
    "for every $\\theta\\in\\Theta_{1}$. For simple versus simple tests, LRT is\n",
    "the uniformly most powerful test according to Neyman-Pearson Lemma.\n",
    "\n",
    "If I am a Bayesian, I do not mind imposing probability (weight) on the\n",
    "parameter space, which is my prior belief $\\pi\\left(\\theta\\right)$. My\n",
    "Bayesian risk becomes $$\\begin{aligned}\n",
    "\\mathscr{BR}\\left(\\pi,\\phi\\right) & =E_{\\pi\\left(\\theta\\right)}\\left[\\mathscr{R}\\left(\\theta,\\phi\\right)\\right]=\\int\\left[\\beta_{\\phi}\\left(\\theta\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{0}\\right\\} +\\left(1-\\beta_{\\phi}\\left(\\theta\\right)\\right)\\cdot1\\left\\{ \\theta\\in\\Theta_{1}\\right\\} \\right]\\pi\\left(\\theta\\right)d\\theta\\\\\n",
    " & =\\int_{\\left\\{ \\theta\\in\\Theta_{0}\\right\\} }\\beta_{\\phi}\\left(\\theta\\right)\\pi\\left(\\theta\\right)d\\theta+\\int_{\\left\\{ \\theta\\in\\Theta_{1}\\right\\} }(1-\\beta_{\\phi}\\left(\\theta\\right))\\pi\\left(\\theta\\right)d\\theta.\\end{aligned}$$\n",
    "This is the average (with respect to $\\pi\\left(\\theta\\right)$) risk over\n",
    "the null and the alternative.\n",
    "\n",
    "**Historical notes**: Hypothesis testing started to take the modern\n",
    "shape at the beginning of the 20th century. Karl Pearson (1957–1936)\n",
    "laid the foundation of hypothesis testing and introduced the $\\chi^{2}$\n",
    "test, the $p$-value, among many other concepts that we keep using today.\n",
    "Neyman-Pearson Lemma was named after Jerzy Neyman (1894–1981) and Egon\n",
    "Pearson (1895–1980), Karl’s son.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478ff54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2a6f1b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d758a6962be9d6930c55e72312cb4ae9df8211d3a6044169c64a6cca0640e9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
