---
title: 'Lecture 2: Expectation'
author: "Zhentao Shi"
date: "August 8, 2018"
output: pdf_document
linestretch: 1.5
fontsize: 12pt
urlcolor: blue
---



# Expected Value {#expectation}


## Integration

Integration is one of the most fundamental operations in mathematical analysis.
We have studied Riemann's integral in the undergraduate calculus.
Riemann's integral is intuitive, but Lebesgue integral is a more general approach to defining integration.

Lebesgue integral is
constructed by the following steps. $X$ is called a *simple function* on a measurable space
    $\left(\Omega,\mathcal{F}\right)$ if
    $X=\sum_{i}a_{i}\cdot 1\left\{ A_{i}\right\}$ and this summation is finite, where
    $a_{i}\in\mathbb{R}$ and $\{ A_i\in\mathcal{F} \}_{i\in \mathbb{N}}$ is a partition
of $\Omega$. A simple function is measurable.


1. Let $\left(\Omega,\mathcal{F},\mu\right)$ be a measure space. The integral of the simple function $X$ with respect to $\mu$
is
$$\int X\mathrm{d}\mu=\sum_{i}a_{i}\mu\left(A_{i}\right).$$
Unlike the Rieman integral, this definition of integration does not
partition the domain into splines of equal length. Instead, it tracks
the distinctive values of the function and the corresponding measure.

2. Let $X$ be a non-negative measurable function. The integral of $X$
    with respect to $\mu$ is
    $$\int X\mathrm{d}\mu=\sup\left\{ \int Y\mathrm{d}\mu:0\leq Y\leq X,\text{ }Y\text{ is simple}\right\} .$$
3. Let $X$ be a measurable function. Define
    $X^{+}=\max\left\{ X,0\right\}$ and
    $X^{-}=-\min\left\{ X,0\right\}$. Both $X^{+}$ and $X^{-}$ are
    non-negative functions. The integral of $X$ with respect to $\mu$ is
    $$\int X\mathrm{d}\mu=\int X^{+}\mathrm{d}\mu-\int X^{-}\mathrm{d}\mu.$$

The Step 1 above defines the integral of a simple function.
Step 2 defines the integral of a non-negative function as the approximation of
steps functions from below.
Step 3 defines the integral of a general function as the difference of the
integral of two non-negative parts.

If the measure $\mu$ is a probability measure $P$, then the integral
    $\int X\mathrm{d}P$ is called the *expected value,* or
    *expectation,* of $X$. We often use the notation
    $E\left[X\right]$, instead of $\int X\mathrm{d}P$, for convenience.

Expectation provides the average of a random variable,
despite that we cannot foresee the realization of a random variable in a particular trial
(otherwise the study of uncertainty is trivial). In the frequentist's view,
the expectation is the average outcome if we carry out a large number of independent
trials.

If we know the probability mass function of a discrete random variable, its expectation
is calculated as $E\left[X\right]=\sum_{x}xP\left(X=x\right)$, which is
the integral of a simple function.
If a continuous random variable has a PDF $f(x)$, its expectation
can be computed as  $E\left[X\right]=\int xf\left(x\right)\mathrm{d}x$.
These two expressions are unified as
$E[X] = \int X \mathrm{d} P$ by  the Lebesgue integral.



Here are some properties of the expectation.


-  The probability of an event $A$ is the expectation
of an indicator function. $E\left[1\left\{ A\right\}  \right]= 1\times P(A) + 0 \times P(A^c) =P\left(A\right)$.

-   $E\left[X^{r}\right]$ is call the $r$-moment of $X$. The *mean* of a random variable is the first moment $\mu=E\left[X\right]$, and
the second *centered* moment is called the *variance*
$\mathrm{var}\left[X\right]=E\left[\left(X-\mu\right)^{2}\right]$.
The third centered moment $E\left[\left(X-\mu\right)^{3}\right]$,
called *skewness*, is a measurement of the
symmetry of a random variable, and the fourth centered moment
    $E\left[\left(X-\mu\right)^{4}\right]$, called *kurtosis*, is
     a measurement of the tail thickness.

- We call
    $E\left[\left(X-\mu\right)^{3}\right]/\sigma^{3}$ the *skewness coefficient*, and
    $E\left[\left(X-\mu\right)^{4}\right]/\sigma^{4}-3$ *degree of excess*. A normal distribution's  skewness and  degree of excess are both zero.

    -   **Application**: [The formula that killed Wall
        Street](http://archive.wired.com/techbiz/it/magazine/17-03/wp_quant?currentPage=all)

- Moments do not always exist. For example, the mean of the Cauchy distribution does not exist,
and the variance of the $t(2)$ distribution do not exist.

- $E[\cdot]$ is a linear operation. If $\phi(\cdot)$ is a linear function, then $E[\phi(X)] = \phi(E[X]).$

-   *Jensen's inequality* is an important fact.
A function $\varphi(\cdot)$ is convex if
$\varphi( a x_1 + (1-a) x_2 ) \leq a \varphi(x_1) + (1-a) \varphi(x_2)$ for all $x_1,x_2$
in the domain and $a\in[0,1]$. For instance, $x^2$ is a convex function.
Jensen's inequality says that if $\varphi\left(\cdot\right)$ is a convex
    function, then
    $\varphi\left(E\left[X\right]\right)\leq E\left[\varphi\left(X\right)\right].$

    -   **Application**: The *Kullback-Leibler divergence* is defined as
    \[
    d\left(P,Q\right)=\int\log\left( \frac{\mathrm{d}P}{\mathrm{d}Q}\right)\mathrm{d}P
    \]
    for two probability measures $P$ and $Q$. The divergence $d(P,Q)\geq 0$ and the inequality holds if and only if  $P=Q$ almost everywhere.  

-   *Markov inequality* is another simple but important fact. If $E\left[\left|X\right|^{r}\right]$ exists,
    then
    $P\left(\left|X\right|>\epsilon\right)\leq E\left[\left|X\right|^{r}\right]/\epsilon^{r}$
    for all $r\geq1$. *Chebyshev inequality*
        $P\left(\left|X\right|>\epsilon\right)\leq E\left[X^{2}\right]/\epsilon^{2}$
        is a special case of the Markov inequality when $r=2$.

- The distribution of a random variable is completely characterized by its
CDF or PDF. Moment is a function of the distribution. To back out the underlying distribution
from moments, we need to know the moment-generating function $M_X(t) = E [ e^{tX}]$ for $t\in \mathbb{R}$
whenever the expectation exists.
\[
E[X^r ] = \frac{ \mathrm{d}^r M_X(t) }{ \mathrm{d} t^r } \bigg \vert_{t=0}
\]

# Multivariate Random Variable

A bivariate random variable is a
measurable function $X:\Omega\mapsto\mathbb{R}^{2}$, and more generally a multivariate random
variable is a measurable function $X:\Omega\mapsto\mathbb{R}^{n}$.
We can define the *joint CDF* as
$F\left(x_{1},\ldots,x_{n}\right)=P\left(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n}\right)$.
Joint PDF is defined similarly.

It is sufficient to introduce the joint distribution, conditional distribution
and marginal distribution in the simple bivariate case, and these definitions
can be extended to multivariate distributions. Suppose
a bivariate random variable $(X,Y)$ has a joint density
$f(\cdot,\cdot)$.
The  *conditional density* can be roughly written as  $f\left(y|x\right)=f\left(x,y\right)/f\left(x\right)$ if we do not formally deal
with the case $f(x)=0$.
The *marginal density* $f\left(y\right)=\int f\left(x,y\right)dx$ integrates out
the coordinate that is not interested.

## Independence

In a probability space $(\Omega, \mathcal{F}, P)$, for two events $A_1,A_2\in \mathcal{F}$ the *conditional probability* is
$$P\left(A_1|A_2\right)=\frac{P\left(A_1 A_2\right)}{P\left(A_2\right)}$$
if $P(A_2) \neq 0$. If $P(A_2) =0$, the conditional probability can still be valid
in some cases,
but we need to introduce the *dominance* between two measures,
which I choose not to do at this time.
In the definition of conditional probability, $A_2$ plays
the role of the outcome space  so that
 $P(A_1 A_2)$ is standardized by the total mass $P(A_2)$.

Since $A_1$ and $A_2$ are symmetric, we also have $P(A_1 A_2) = P(A_2|A_1)P(A_1)$.
It implies
$$P(A_1 | A_2)=\frac{P\left(A_2| A_1\right)P\left(A_1\right)}{P\left(A_2\right)}$$
This formula is the well-known *Bayes' Theorem*. It is particularly important in
decision theory.

**Example:** $A_1$ is the event "a student can survive CUHK's MSc program", and $A_2$ is
his or her application profile.

We say two events $A_1$ and $A_2$ are *independent* if $P(A_1A_2) = P(A_1)P(A_2)$.
If $P(A_2) \neq 0$, it is equivalent to $P(A_1 | A_2 ) = P(A_1)$.
In words, knowing $A_2$ does not change the probability of $A_1$.

Regarding the independence of two random variables,
$X$ and $Y$ are *independent* if
$P\left(X\in B_1,Y\in B_2\right)=P\left(X\in B_1\right)P\left(Y\in B_2\right)$
for any two Borel sets $B_1$ and $B_2$.

If $X$ and $Y$ are independent, $E[XY] = E[X]E[Y]$.

**Application**: (Chebyshev law of large numbers) If
$X_{1},X_{2},\ldots,X_{n}$ are independent, and they have the same mean
$0$ and variance $\sigma^{2}<\infty$. Let
$Z_{n}=\frac{1}{n}\sum_{i=1}^{n} X_{i}$. Then the
probability $P\left(\left|Z_{n}\right|>\epsilon\right)\to0$ as
$n\to\infty$.

The culmination of probability theory is *law of large numbers* and
*central limit theorem*.

## Law of Iterated Expectations


Given a probability space $\left(\Omega,\mathcal{F},P\right)$, a sub
    $\sigma$-algebra $\mathcal{G}\subset\mathcal{F}$ and a
    $\mathcal{F}$-measurable function $X$ with $E\left|X\right|<\infty$,
    the *conditional expectation* $E\left[X|\mathcal{G}\right]$ is
    defined as a $\mathcal{G}$-measurable function such that
    $\int_{A}XdP=\int_{A}E\left[X|\mathcal{G}\right]dP$ for all
    $A\in\mathcal{G}$. *Law of iterated expectation* is a trivial fact if we take
    $A=\Omega$.



In the bivariate case, if the conditional density exists, the conditional expectation can be computed as
    $E\left[Y|X\right]=\int yf\left(y|X\right)dy$.
The law of iterated expectation implies $E\left[E\left[Y|X\right]\right]=E\left[Y\right]$.


Below are some properties of conditional expectations

1.  $E\left[E\left[Y|X_{1},X_{2}\right]|X_{1}\right]=E\left[Y|X_{1}\right];$
2.  $E\left[E\left[Y|X_{1}\right]|X_{1},X_{2}\right]=E\left[Y|X_{1}\right];$
3.  $E\left[h\left(X\right)Y|X\right]=h\left(X\right)E\left[Y|X\right].$

**Application**: Regression is a technique that decomposes a random variable $Y$
into two parts, a conditional mean and a residual. Write
 $Y=E\left[Y|X\right]+\epsilon$, where
$\epsilon=Y-E\left[Y|X\right]$. Show that $E[\epsilon] = 0$ and  $E[\epsilon E[Y|X] ] = 0$.
