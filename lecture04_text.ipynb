{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20031a6f",
   "metadata": {},
   "source": [
    "\n",
    "Suppose the data is generated from a parametric model. Statistical\n",
    "estimation looks for the unknown parameter from the observed data. A\n",
    "*principle* is an ideology about a proper way of estimation. Over the\n",
    "history of statistics, only a few principles are widely accepted. Among\n",
    "them Maximum Likelihood is the most important and fundamental. The\n",
    "maximum likelihood principle entails that the unknown parameter being\n",
    "found as the maximizer of the log-likelihood function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d6952",
   "metadata": {},
   "source": [
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "In this chapter, we first give an introduction of the maximum likelihood\n",
    "estimation. Consider a random sample of\n",
    "$Z=\\left(z_{1},z_{2},\\ldots,z_{n}\\right)$ drawn from a parametric\n",
    "distribution with density $f_{z}\\left(z_{i};\\theta\\right)$, where\n",
    "$z_{i}$ is either a scalar random variable or a random vector. A\n",
    "parametric distribution is completely characterized by a\n",
    "finite-dimensional parameter $\\theta$. We know that $\\theta$ belongs to\n",
    "a parameter space $\\Theta$. We use the data to estimate $\\theta$.\n",
    "\n",
    "The log-likelihood of observing the entire sample $Z$ is\n",
    "$$L_{n}\\left(\\theta;Z\\right):=\\log\\left(\\prod_{i=1}^{n}f_{z}\\left(z_{i};\\theta\\right)\\right)=\\sum_{i=1}^{n}\\log f_{z}\\left(z_{i};\\theta\\right).$$\n",
    "In reality the sample $Z$ is given and for each $\\theta\\in\\Theta$ we can\n",
    "evaluate $L_{n}\\left(\\theta;Z\\right)$. The maximum likelihood estimator\n",
    "is\n",
    "$$\\widehat{\\theta}_{MLE}:=\\arg\\max_{\\theta\\in\\Theta}L_{n}\\left(\\theta;Z\\right).$$\n",
    "Why maximizing the log-likelihood function is desirable? An intuitive\n",
    "explanation is that $\\widehat{\\theta}_{MLE}$ makes observing $Z$ the\n",
    "“most likely” in the entire parametric space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8e703",
   "metadata": {},
   "source": [
    "\n",
    "Consider the Gaussian location model $z_{i}\\sim N\\left(\\mu,1\\right)$,\n",
    "where $\\mu$ is the unknown parameter to be estimated. The likelihood of\n",
    "observing $z_{i}$ is\n",
    "$f_{z}\\left(z_{i};\\mu\\right)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)$.\n",
    "The likelihood of observing the sample $Z$ is\n",
    "$$f_{Z}\\left(Z;\\mu\\right)=\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(z_{i}-\\mu\\right)^{2}\\right)$$\n",
    "and the log-likelihood is\n",
    "$$L_{n}\\left(\\mu;Z\\right)=-\\frac{n}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.$$\n",
    "The (averaged) log-likelihood function for the $n$ observations is\n",
    "$$\\begin{aligned}\n",
    "\\ell_{n}\\left(\\mu\\right) & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2n}\\sum_{i=1}^{n}\\left(z_{i}-\\mu\\right)^{2}.\\end{aligned}$$\n",
    "We work with the averaged log-likelihood $\\ell_{n}$, instead of the\n",
    "(raw) log-likelihood $L_{n}$, to make it directly comparable with the\n",
    "expected log density $$\\begin{aligned}\n",
    "E_{\\mu_{0}}\\left[\\log f_{z}\\left(z;\\mu\\right)\\right] & =E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu\\right)^{2}\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(\\left(z_{i}-\\mu_{0}\\right)+\\left(\\mu_{0}-\\mu\\right)\\right)^{2}\\right]\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}E_{\\mu_{0}}\\left[\\left(z_{i}-\\mu_{0}\\right)^{2}\\right]-E_{\\mu_{0}}\\left[z_{i}-\\mu_{0}\\right]\\left(\\mu_{0}-\\mu\\right)-\\frac{1}{2}\\left(\\mu_{0}-\\mu\\right)^{2}\\\\\n",
    " & =-\\frac{1}{2}\\log\\left(2\\pi\\right)-\\frac{1}{2}-\\frac{1}{2}\\left(\\mu-\\mu_{0}\\right)^{2}.\\end{aligned}$$\n",
    "where the first equality holds because of random sampling. Obviously,\n",
    "$\\ell_{n}\\left(\\mu\\right)$ is maximized at\n",
    "$\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$ while\n",
    "$E_{\\mu_{0}}\\left[\\ell_{n}\\left(\\mu\\right)\\right]$ is maximized at\n",
    "$\\mu=\\mu_{0}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea45be7",
   "metadata": {},
   "source": [
    "\n",
    "We use the following code to demonstrate the population log-likelihood\n",
    "$E\\left[\\ell_{n}\\left(\\mu\\right)\\right]$ when $\\mu_{0}=2$ (solid line)\n",
    "and the 3 sample realizations when $n=4$ (dashed lines).\n",
    "\n",
    "\\*\\*there is a knitr\\*\\* part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5485b5c",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "The exact distribution under the normality assumption of the error term\n",
    "is the classical statistical results. The Gauss Markov theorem holds\n",
    "under two crucial assumptions: linear CEF and homoskedasticity.\n",
    "\n",
    "**Historical notes**: MLE was promulgated and popularized by Ronald\n",
    "Fisher (1890–1962). He was a major contributor of the frequentist\n",
    "approach which dominates mathematical statistics today, and he sharply\n",
    "criticized the Bayesian approach. Fisher collected the iris flower\n",
    "dataset of 150 observations in his biological study in 1936, which can\n",
    "be displayed in R by typing `iris`. Fisher invented the many concepts in\n",
    "classical mathematical statistics, such as sufficient statistic,\n",
    "ancillary statistic, completeness, and exponential family, etc.\n",
    "\n",
    "**Further reading**: @phillips1983exact offered a comprehensive\n",
    "treatment of exact small sample theory in econometrics. After that,\n",
    "theoretical studies in econometrics swiftly shifted to large sample\n",
    "theory, which we will introduce in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1b167",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix\n",
    "\n",
    "### Joint Normal Distribution\n",
    "\n",
    "It is arguable that normal distribution is the most frequently\n",
    "encountered distribution in statistical inference, as it is the\n",
    "asymptotic distribution of many popular estimators. Moreover, it boasts\n",
    "some unique features that facilitates the calculation of objects of\n",
    "interest. This note summaries a few of them.\n",
    "\n",
    "An $n\\times1$ random vector $Y$ follows a joint normal distribution\n",
    "$N\\left(\\mu,\\Sigma\\right)$, where $\\mu$ is an $n\\times1$ vector and\n",
    "$\\Sigma$ is an $n\\times n$ symmetric positive definite matrix. The\n",
    "probability density function is\n",
    "$$f_{y}\\left(y\\right)=\\left(2\\pi\\right)^{-n/2}\\left(\\mathrm{det}\\left(\\Sigma\\right)\\right)^{-1/2}\\exp\\left(-\\frac{1}{2}\\left(y-\\mu\\right)'\\Sigma^{-1}\\left(y-\\mu\\right)\\right)$$\n",
    "where $\\mathrm{det}\\left(\\cdot\\right)$ is the determinant of a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc3248",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will discuss the relationship between two components of a random\n",
    "vector. To fix notation, $$Y=\\left(\\begin{array}{c}\n",
    "Y_{1}\\\\\n",
    "Y_{2}\n",
    "\\end{array}\\right)\\sim N\\left(\\left(\\begin{array}{c}\n",
    "\\mu_{1}\\\\\n",
    "\\mu_{2}\n",
    "\\end{array}\\right),\\left(\\begin{array}{cc}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{array}\\right)\\right)$$\n",
    "where $Y_{1}$ is an $m\\times1$ vector, and\n",
    "$Y_{2}$ is an $\\left(n-m\\right)\\times1$ vector. $\\mu_{1}$ and $\\mu_{2}$\n",
    "are the corresponding mean vectors, and $\\Sigma_{ij}$, $j=1,2$ are the\n",
    "corresponding variance and covariance matrices. From now on, we always\n",
    "maintain the assumption that $Y=\\left(Y_{1}',Y_{2}'\\right)'$ is jointly\n",
    "normal.\n",
    "\n",
    "Fact immediately implies a convenient feature of the normal distribution.\n",
    "Generally speaking, if we are given a joint pdf of two random variables\n",
    "and intend to find the marginal distribution of one random variables, we\n",
    "need to integrate out the other variable from the joint pdf. However, if\n",
    "the variables are jointly normal, the information of the other random\n",
    "variable is irrelevant to the marginal distribution of the random\n",
    "variable of interest. We only need to know the partial information of\n",
    "the part of interest, say the mean $\\mu_{1}$ and the variance\n",
    "$\\Sigma_{11}$ to decide the marginal distribution of $Y_{1}$.\n",
    "\n",
    "<span id=\"fact:marginal\"\n",
    "label=\"fact:marginal\">\\[fact:marginal\\]</span>The marginal distribution\n",
    "$Y_{1}\\sim N\\left(\\mu_{1},\\Sigma_{11}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea5ae2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
